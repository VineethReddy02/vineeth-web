[{"categories":null,"contents":"","permalink":"https://vineethweb.com/publications/alldaydevops/","tags":[""],"title":""},{"categories":null,"contents":"The motive behind this plugin is to manage set of kubernetes clusters with a kubectl plugin. All the kubectl actions can be performed using this plugin. The plugin cascades the performed operation to all the k8s clusters.\n","permalink":"https://vineethweb.com/projects/creations/kubectl-mc/","tags":["Kubernetes"],"title":"kubectl mc to manage multiple clusters"},{"categories":null,"contents":" \nHelm is a package manager for Kubernetes. A Chart is a Helm package. It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an Apt dpkg, or a Yum RPM file.\nA Release is an instance of a chart running in a Kubernetes cluster. One chart can often be installed multiple times into the same cluster. And each time it is installed, a new release is created.\nWhy we need helm?  Ship prepackaged software. Easily install packages into any Kubernetes cluster. Easily create \u0026amp; host your own software as helm charts. Visibility in cluster to see what packages are installed \u0026amp; running. Update, delete, rollback, or view the history of installed packages.  Helm 2 vs Helm 3  No more tiller: Initially when helm was developed there was no strong access control model in Kubernetes. From Kubernetes 1.6 release Role Based Access Control (RBAC) has become default acess control. In helm 3 the access control is handed over to RBAC and we don\u0026rsquo;t need tiller anymore. Two-way to Three-way strategic merge patch: In helm 2 if we come across the scenario like we deployed an application using helm chart and we made changes on configuration in live cluster using kubectl edit and for some reason we decide to rollback. we don\u0026rsquo;t consider the live state of cluster we compare the previous charts and new charts. In helm 3 you are covered the three way merge strategy works my taking previous helm chart, live state of the application and latest chart into consideration and performs a three way merge strategy. Release info will be stored as secrets: In helm 2 all the helm installed related release information was stored in configmaps which needs additional encryption \u0026amp; decryption to avoid complexity around storing this information in helm 3 release information will be stored as secrets. Release name is required or use \u0026ndash;generate-name flag: Helm 2 has by default random release name generation if user doesn\u0026rsquo;t provide one. But in helm 3 release name is required if not provided helm throws an error. If you want helm to generate one use --generate-name flag.\n Local chart repository is removed i.e helm serve: For local development purposes in helm 2 there was helm serve which was used to run a local Chart Repository on your machine in helm 3 this has been removed but available as a plugin. Namespaces are not created automatically: In helm 2 if values.yaml contains a namespace which doesn\u0026rsquo;t exist in the cluster it used to create namespace automatically before creating the resources. But in helm 3 this will cause an error before installing the helm chart we need to create namespace if it doesn\u0026rsquo;t exist or use --create-namespace flag. JSON Schema Chart Validation: In helm 3 you can validate the values provided by the user with the schema created by the chart maintainer. This provides better error reporting when we mess up with values.  Helm Hub Helm hub is a centralized location where all the communtiy developed helm charts are maintained. We do have other helm chart respositories like bitnami https://github.com/bitnami/charts etc\u0026hellip;\nYou can also host your own helm chart repository for your helm charts example helm chart repository chartmuseum https://chartmuseum.com/\nDirectory structure of a helm chart └── helm-demo ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── hpa.yaml │ ├── ingress.yaml │ ├── NOTES.txt │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests │ └── test-connection.yaml ├── values.yaml └── values.schema.json Charts: Charts directory contains all the charts upon which this chart depends.\nChart.yaml: A yaml file containing information about the chart.\ntemplates: When the temaplte files are combined with values the generated manifest files are stored in templates directory.\nvalues.schema.json: A json schema for imposing the structure and validations on the values.yaml file. (optional)\nIn helm all the magic works using templating the two main files we should care about are values.yaml and all manifests files under templates directory. Define all the values in values.yaml file and use yaml templating for referencing the values and for re-using the values across template files. You can also override the flags while installing the helm chart by using flags such as --set, --set-string and -f for passing your own values.yaml file.\nAvailable CLI commands completion generate autocompletions script for the specified shell (bash or zsh) create create a new chart with the given name dependency manage a chart\u0026#39;s dependencies env helm client environment information get download extended information of a named release help Help about any command history fetch release history install install a chart lint examines a chart for possible issues list list releases package package a chart directory into a chart archive plugin install, list, or uninstall Helm plugins pull download a chart from a repository and (optionally) unpack it in local directory repo add, list, remove, update, and index chart repositories rollback roll back a release to a previous revision search search for a keyword in charts show show information of a chart status displays the status of the named release template locally render templates test run tests for a release uninstall uninstall a release upgrade upgrade a release verify verify that a chart at the given path has been signed and is valid version print the client version information Creating your first helm chart The below command creates the helm chart with all the basic templating.\nhelm create \u0026lt;chart_name\u0026gt; Installing the helm chart The below command installs the helm chart. Release name is specific to a namespace and it should be unique.\nhelm install \u0026lt;release_name\u0026gt; \u0026lt;chart_name\u0026gt; Installing an helm chart from helm hub Example:\nInitialising a helm chart repo\nhelm repo add stable https://kubernetes-charts.storage.googleapis.com/ Search for the desired chart\nhelm search repo stable/cassandra Installing the helm chart\nhelm install cassandra01 stable/cassandra Uninstalling a helm chart The helm uninstall command helps you to delete all the kubernetes resources deployed by installing a specific chart\nhelm uninstall \u0026lt;chart_release_name\u0026gt; Upgrade \u0026amp; Rollback a release using a helm chart After making necessary changes to helm chart please update app version if image tag has been updated or if there any chnages around helm chart configuration update the chart version.\nUpgrading a release will add an revision to the release which is useful while rolling back the upgrade.\nhelm upgrade \u0026lt;release_name\u0026gt; \u0026lt;chart_name\u0026gt; Rolling back an upgrade\nhelm rollback \u0026lt;release_name\u0026gt; \u0026lt;revision_number\u0026gt; To view manifest files helm template \u0026lt;chart_name\u0026gt; Here is an basic helm chart which has templating for a deployment to one of my project called mocklet https://github.com/VineethReddy02/mocklet-helm\nCheers \u0026amp; Happy Helming!\n","permalink":"https://vineethweb.com/blog/helm/","tags":["Kubernetes"],"title":"Helming"},{"categories":null,"contents":"Cortex, being a Distributed, Multi-Tenant Prometheus off-loads the storage to battle-tested databases like Cassandra, BigTable, etc. All the time-series data is converted into chunks. The chunk store essentially contains two parts i.e Index Store, where index to the chunks are stored \u0026amp; KV Store for the chunks itself for direct access. For Index store, There are multiple types of schemas supported by passing the relevant config through chunk.SchemaConfig. Right now, there are only some storage backends supported but as we are seeing more and more users adopting Cortex in their companies,iIt’s important that there is a way for them to extend Cortex to make it use their existing back-ends for storage.\n Provide a way for users to write out-of-tree Chunk Store Implementations. Use standard protocols like gRPC for communication. Provide a new client which talks to the new storage systems. Provide easily configurable storage plugins.  ","permalink":"https://vineethweb.com/projects/contributions/grpc-store-cortex/","tags":["Monitoring","Distributed Systems"],"title":"gRPC store for Cortex"},{"categories":null,"contents":" \nThe most common problem kubernetes controllers and operators authors face today is to benchmark their tools in large environments with 1000\u0026rsquo;s of workloads running in it. As controllers \u0026amp; operators come with custom logic to perform an action when a specific event occurs in the cluster to achieve the intended state.\nCreating 100 node cluster for benchmarking the Kubernetes tools is definitely expensive. Creation, maintainance \u0026amp; deletion is also a tiring job. What if we can create a mock kubelet for scale tests by just running a deployment in your cluster? Yes, This is possible and you can leverage out of this tool in benchmarking the kubernetes tools.\nAll we need is to simulate loaded environments to perform scale test on our tools. Creating loaded environemnts comes with cost. And we don\u0026rsquo;t want to spend that huge figures in benchmarking our tools.\nWhen I refer to Kubernetes tools, I mean the Kubernetes Controllers, Operators, kubectl plugins, etc.. which interact with api-server for events and perform kubernetes resource scheduling.\nmocklet So I will be explaining how to simulate a mocklet that can hold 1000\u0026rsquo;s pods in it\u0026rsquo;s memory and this mock kubelet can be connected to your existing cluster by just running a deployment in your cluster.\nRunning the below command will create a new mocklet node in your cluster\nkubectl create -f https://github.com/VineethReddy02/mocklet/blob/master/k8s-deployment.yaml Now you can notice mocklet is added into your cluster\nNAME STATUS ROLES AGE VERSION gke-gke5684-default-pool-1y5e7l53-kphx Ready \u0026lt;none\u0026gt; 4h23m v1.14.10-gke.27 gke-gke5684-default-pool-1y5e7l53-x5kj Ready \u0026lt;none\u0026gt; 4h23m v1.14.10-gke.27 mocklet Ready agent 2m32s v1.15.2-vk-N/A Creating multiple deployments will create multiple mocklets in your cluster to run desired number of kubernetes resources and scheduling resources specific to a mocklet.\nNow you can deploy 1000\u0026rsquo;s of pods by providing the node selector value as mocklet and mocklet toleration. This will make sure all the test data you are creating is scheduled on the desired mocklet.\nThe mocklet project is completely inspired from Virtual Kubelet mock provider. Thanks to the Virtual Kubelet community.\nThe bigger challenge is how do I create 1000\u0026rsquo;s of pods, Deployments, Replicasets, ReplicationControllers, StatefulSets, Jobs and Cronjobs?\nk8s-scaler To create the Kubernetes resources at scale in a single shot. I have implemented tool called k8s-scaler. This tool is highly configurable and helps you to create Kubernetes resources which runs pods eventually with higly configurable properties such as number of containers, inclusion \u0026amp; exlusion of namespaces during resource creation, number of Kubernetes resources like deployments, daemonsets, etc\u0026hellip; , number of replicas per Kubernetes controller.\nCreating 5000 deployments with replica count as 5 per instance and number of containers per pod as 3 in scale namespace with node-selector \u0026amp; toleration is as simple as\n./k8s-scaler create deployments --scale 5000 --replicas 5 --containers 3 --namespace scale --node-selector type=mocklet --toleration mocklet.io/provider=mock Note: Using k8s-scaler you can also create/delete namespaces, daemonsets, statefulsets, replicationcontrollers, replicasets, jobs and cronjobs\nk8s-scaler also helps you in listing number of kubernetes resources per namespace as shown below.\nvineeth@vineeth-Latitude-7490 /bin (master) $ ./k8s-scaler list NAMESPACE DEPLOYMENTS REPLICASETS DAEMONSETS STATEFULSETS PODS JOBS CRONJOBS REPLICATION-CONTROLLERS test 3000 3000 1000 500 7486 30 10 30 default 1300 1300 456 250 5642 10 5 5 kube-system 8 11 4 0 15 0 0 0 mocklet 3500 4000 1200 400 9348 50 30 35  Using the mocklet and k8s-scaler you can create the large environments with ease. Running mocklet will provide the mock kubelet to run the desired number of resources and using k8s-scaler you can create desired number of kubernetes resources by running a single command in any kubernetes cluster.\nI hope these tools will help you in scale testing the tools built around kubernetes.\nCheers!\n","permalink":"https://vineethweb.com/blog/scale-testing-in-k8s/","tags":["kubernetes"],"title":"Benchmarking your Controllers \u0026 Operators at Scale"},{"categories":null,"contents":"\n","permalink":"https://vineethweb.com/certifications/ckad/","tags":["Kubernetes"],"title":"Certified Kubernetes Application Developer (CKAD)"},{"categories":null,"contents":"","permalink":"https://vineethweb.com/projects/contributions/k8s-release/","tags":["kubernetes"],"title":"Docs team lead for Kubernetes 1.18 release"},{"categories":null,"contents":"\n\nSecurity in the world of containers has become complex as we use different base images. We cannot be aware of what our containers are made up of and cope up with dependencies and transitive dependencies and their updates. On average at least 30 vulnerabilities exist in the top 10 docker images. Do you know if you’re using any of these? If your application containers are based on vulnerable images, your deployment could be open to attack. In this talk, you’ll learn about practical actions to address vulnerabilities in your container images. I will be demonstrating securing the images in a Harbor registry by integrating the open-source Trivy image scanning tool. Trivy is very easy to use and can also be integrated with the existing CI/CD pipelines. After this session, attendees will take away the best practices in securing their container workloads.\nLink to the talk: Securing Container Workloads\n","permalink":"https://vineethweb.com/talks/k8s-forum-delhi/","tags":["Docker","Containers","Security"],"title":"Securing Container Workloads at K8s Forum Delhi 2020"},{"categories":null,"contents":"\n","permalink":"https://vineethweb.com/certifications/cka/","tags":["Kubernetes"],"title":"Certified Kubernetes Administrator (CKA)"},{"categories":null,"contents":" gRPC gRPC is a RPC platform initially developed by Google and now a project under CNCF. The letters gRPC are a recursive acronym which means, gRPC Remote Procedure Call.\ngRPC has two parts, the gRPC protocol, and the data serialization. By default gRPC utilizes Protobuf for serialization, but it is pluggable with any form of serialization you wish to use, with some caveats.\nHTTP/2 gRPC supports several built in features inherited from http2, such as compressing headers, persistent single TCP connections, cancellation and timeout contracts between client and server.\nStreams, Messages, and Frames The introduction of the new binary framing mechanism changes how the data is exchanged between the client and server. To describe this process, let’s familiarize ourselves with the HTTP/2 terminology:\nStream: A bidirectional flow of bytes within an established connection, which may carry one or more messages.\nMessage: A complete sequence of frames that map to a logical request or response message.\nFrame: The smallest unit of communication in HTTP/2, each containing a frame header, which at a minimum identifies the stream to which the frame belongs.\n All communication is performed over a single TCP connection that can carry any number of bidirectional streams.\n Each stream has a unique identifier and optional priority information that is used to carry bidirectional messages.\n Each message is a logical HTTP message, such as a request, or response, which consists of one or more frames.\n The frame is the smallest unit of communication that carries a specific type of data—e.g., HTTP headers, message payload, and so on. Frames from different streams may be interleaved and then reassembled via the embedded stream identifier in the header of each frame.\n  gRPC offers:  Request \u0026amp; Response Multiplexing Stream Prioritization One connection Per Origin Flow Control Server Push Header Compression  HTTP/2 vs HTTP 1.1 Comparing REST and gRPC. REST, as mentioned earlier, depends heavily on HTTP (usually HTTP 1.1) and the request-response model. On the other hand, gRPC uses the newer HTTP/2 protocol.\nThe probelms with HTTP 1.1 that HTTP/2 fixes.\n HTTP 1.1 is too big and complicated The Growth of Page Size and Number of Objects Latency issues Head of Line Blocking (reduces the ability to process parallel requests)  Default choice - REST\nFor simple services, HTTP REST is probably enough\n HTTP verbs (GET, POST, PUT, DELETE etc.) are rich enough REST semantics are well understood.  When not to pick REST?\nFor more complex services where efficiency is important, RPC can help.\nPRO\u0026rsquo;s:\n SIMPLE \u0026amp; IDIOMATIC PERFORMANT \u0026amp; SCALABLE INTEROPERABLE \u0026amp; EXTENSIBLE  Google uses this framework to make 10 the power 10 calls per second.\nHOL The probelem raises in HTTP/1 with HOL (Head of line blocking) when we sent a request to the server now on the connection is useless until it returns the response. Originally the server has allowed two connections per server had reuse the connection once the reponse has received. So this bottleneck was addressed by raising the limit to 6 connections.\nMetadata Headers repeat alot across the request. The headers that repeat are user-agents \u0026amp; cookies ther are long and static and they are keep being added for each and every request. This is because HTTP was designed to be completely stateless and independent from the next. And then people started using sessions and these are identified with a long UUID and they are appended to every request being sent over \u0026amp; over which is waste of bandwidth. Though the headers are highly compressable they cannot be gzipped as data which is a actually a miss opportunity.\nHTTP/2 follows the semantics of HTTP/1 this requets contain the headers i.e key/value pairs and body conetent which doesn\u0026rsquo;t need any code related changes. The changes involve in how they are encoded in wire in transit(i.e into binary level)\nThe scariest part is upgrade to HTTP/2 will I loose my clients ? Answer is no.\nSo every connections starts out as H1 and then it upgrades to H2 and if this client doesn\u0026rsquo;t support H2 it will stay on H1 and everything will work as before.\nWhat is HTTP/2 exactly? It\u0026rsquo;s a single TLS encrypted connection. So HOL blocking is addressed at protocol level by single connection.\nLimitations with HTTP/1.1  HTTP 1.1 opens a new TCP connection to a server at each request. It does not compress headers (which are plaintext). It only works with Request/Response mechanism (no server push). These inefficiences add more latency and increase network packet size.  Advantages with HTTP/2  The client \u0026amp; server can push messages in parallel over the same TCP connection This greatly reduces latency. Server can push streams (multiple messages) for one request from the client which will reduce the round trips between client and server. HTTP/2 supports header compressions, This have much less impact on packet size(less bandwidth. Average http request may have over 20 headers, due to cookies,content cache and application headers. HTTP/2 is binary while HTTP/1 is text which is not efficient over the network. Protocol buffers is a binary protocol and makes it a great match for HTTP/2 HTTP/2 is secure (SSL is not required but recommended by default)  Types of API in gRPC  Unary default client to server request/response based communication. Server Streaming client request server and server responds back with stream of responses. Client Streaming client creates a streaming request connection with server and server sends back a single response. Bi Directional Streaming client sends a stream of requests to server and server sends back stream of responses back to client.   Scalability in gRPC  gRPC servers are asynchronous by default. This means they do not block threads on request. Therefore each gRPC server can serve millions of requets in parallel. gRPC Clients can be asynchronous or synchronous (blocking). The client decides which model works best for the performance needs. gRPC Clients can perform client side load balancing.  gRPC vs REST gRPC\n Protocol Buffers - smaller,faster. HTTP/2(lower latency). Bidirectional \u0026amp; Async. Stream Support. API oriented (no constraints-free design). Code Generation through Protocol Buffers in any language-1st class citizen. RPC based gRPC does the plumbing for us.  REST\n JSON- text based,slower, bigger HTTP1.1 (higher latency) Client -\u0026gt; Server requets only Request/Response support only CRUD oriented(Create-Retrieve-Update-Delete/POST GET PUT DELETE) Code generation through OpenAPI/Swagger(add-on)-2nd class citizen HTTP verns based- we have to write the plumbing or use a 3rd party library  Protobuf vs JSON One of the biggest differences between REST \u0026amp; gRPC is the format of the payload. REST messages typically contain JSON. The whole REST ecosystem including tooling, best practices, and tutorials is focused on JSON. It is safe to say that, with very few exceptions, REST APIs accept and return JSON.\ngRPC, on the other hand, accepts and returns Protobuf messages. I will discuss the strong typing later, but just from a performance point of view, Protobuf is a very efficient and packed format. JSON, on the other hand, is a textual format. You can compress JSON, but then you lose the benefit of a textual format that you can easily expect.\nStrong Typing vs. Serialization The REST paradigm doesn\u0026rsquo;t mandate any structure for the exchanged payload. It is typically JSON. Consumers don\u0026rsquo;t have a formal mechanism to coordinate the format of requests and responses. The JSON must be serialized and converted into the target programming language both on the server side and client side. The serialization is another step in the chain that introduces the possibility of errors as well as performance overhead.\nThe gRPC service contract has strongly typed messages that are converted automatically from their Protobuf representation to your programming language of choice both on the server and on the client.\nJSON, on the other hand, is theoretically more flexible because you can send dynamic data and don\u0026rsquo;t have to adhere to a rigid structure.\nServer Streaming  Server Streaming RPC API are a new kind API enabled thanks to HTTP/2 The client will send one message to the server and will receive many responses from the server, possibly an infinite number.  Usecases:\n When the server needs to send a lot of data (big data). When the server needs to PUSH data to the client without having the client request for more.  Client Streaming  Client streaming RPC API are a NEW kind API enabled thanks to HTTP/2 The client will send many message to the server and will receive one response from the server (at any time)\n Streaming client are well suited for\n When the client needs to send a lot of data(big data) When the server processing is expensive and should happen as the client sends data When the client needs to PUSH data to the server without really expecting a response.  In gRPC client streaming calls are defined using the keyword \u0026ldquo;stream\u0026rdquo;\n As for each RPC call we have to define a \u0026ldquo;Request\u0026rdquo; message and a \u0026ldquo;Response\u0026rdquo; message.\n  Bi Directional Streaming API  Bi Directional Streaming RPC API are a new kind API enabked thanks to HTTP/2 The client will send many message to the server and will receive many responses from the server. The number of requests and responses does not have to match\n Bi Directional Streaming RPC are well suited for\n When the client and the server needs to send a lot of data asynronously Chat protocol Long running connections  In gRPC Bi Directional Streaming API are defined using the keyword \u0026ldquo;stream\u0026rdquo;, twice\n As for each RPC call we have to define a \u0026ldquo;Request\u0026rdquo; message and a \u0026ldquo;Response\u0026rdquo; message.w\n  Below is the .proto file which contains the message definitions \u0026amp; stream declarations. syntax = \u0026#34;proto3\u0026#34;; package registration; option go_package=\u0026#34;registration\u0026#34;; message register_request { string name = 1; string email = 2; } message register_response { string registration_id = 1; } message bulk_register_response { repeated register_response bulk_response = 1; } message nothing {} service Registration_service { // Unary  rpc register(register_request) returns (register_response) {}; // Client streaming  rpc register_bulk(stream register_request) returns (bulk_register_response) {}; // Server streaming  rpc get_registered_data(nothing) returns (stream register_response) {}; // client/server streaming  rpc register_multiple_requests(stream register_request) returns (stream register_response) {}; } In Protocol buffers to consume the ablove defined messages \u0026amp; services. We generate the code using the below command\nprotoc registration.proto --go_out=plugins=grpc:. Status Codes in gRPC Codes in gRPC are imported from a package\ngoogle.golang.org/grpc/codes Example:\nBelow are some of the status codes defined in grpc/codes.\nconst ( OK Code = 0 Canceled Code = 1 Unknown Code = 2 ) Streaming with timeout  We can set the timeout per request and cancel the request if the server is taking longer the set up timeout. The timeout context is applicable for chain of microservice requests.  SSL Encryption in gRPC  In producrion gRPC calls should be running with encryption enabled This is done by generating SSL certificates. SSL allows communication to be secure end to end and ensuring no man in the middle attck can be performed.  SSL Security What is SSL?\n TLS (Transport layer security), successor of SSL, encrypts the connection between 2 endpoints for secure data exchange.\n Two ways of using SSL (gRPC can use booth):\n One way verification e.g browser -\u0026gt; webserver Two way verification e.g: SSL authentication   Language Interoperatability In gRPC interoperatability is possible go server can serve the requets to java client and vice versa.\ngRPC Reflection \u0026amp; evans cli We may ask server what API\u0026rsquo;s do you have? That\u0026rsquo;s reflection.\nWe want reflection for two reasons - Having servers expose which endpoints are available. - Allowing command line interfaces (CLI) to talk to our server without preliminary .proto file.\nThe below code snippet allows to expose endpoints of the respective server.\n\u0026#34;google.golang.org/grpc/reflection\u0026#34; reflection.Register(s) Evans cli for grpc helps us to see all the endpoints exposed using command line tool.\nDownload evans cli tool from here:\nhttps://github.com/ktr0731/evans/releases\n#To connect to the gRPC server using evans cli ./evans -p 6666 -r #Shows the package in protocol buffers show package #Shows all the available services in the gRPC server. show service #Shows all the messages defined in protocol buffers. show message #Test the respective service in interactive mode. call register  Demo App Let\u0026rsquo;s create a server that registers the client requests and displays a message saying registration successsful. When client sends register request to server it returns a unique id is returned, on successful registration.\nCheckout the demo app on gRPC here\n","permalink":"https://vineethweb.com/blog/grpc/","tags":["Micro-services","Networking"],"title":"Understanding gRPC"},{"categories":null,"contents":" \nRole Based Access Control in Kubernetes As we all know role based access control plays a vital role. when we have multiple people using the same resources for different purposes. We need to handle this scenarios by providing the users unique identification mechanism and privileges required for that particular user.\nRBAC in kubernetes adheres to least privilege principle.\nNeed for RBAC  As Kubernetes has tens of resources with many functionalities. Different users use them for different use cases. Unnecessary privileges leads to increasing the attack surface.\n Like every other role based access control system kubernetes also deals with authentication and authorization.\n  Authentication deals with does a specific user, group or a service from a machine can authenticate or access this system.\nAuthorization deals with does a specific user have an required privileges to perform a particular action on a specific resource or subresource or group of resources.\nRBAC mainly deals with three entities\n subject: Users(i.e humans \u0026amp; services) verb: Action a subject can perform object: Victim for an action from a subject  Can mike(subject) get(verb) pods(object)?\nKubernetes provides following resources to manage RBAC:\n Role ClusterRole RoleBinding ClusterRoleBinding ServiceAccount  Role: Roles defines the set of privileges allowed on specific resource by it\u0026rsquo;s name or on group of resources or set of subresourcesand this is confined to a specific namespace.\nClusterRole: ClusterRole defines the set of privileges allowed on specific resource by it\u0026rsquo;s name or on group of resources or set of subresources and this is applicable on cluster as a whole.\nRoleBinding: Rolebinding binds the user or group or serviceaccount to a role. This makes sure the subject provided in rolebinding has all the privileges provided within a role.\nClusterRoleBinding: ClusterRolebinding binds the user or group or serviceaccount to a role. This makes sure the subject provided in clusterrolebinding has all the privileges provided within a clusterrole.\nServiceAccount: Serviceaccount authorizes account to perform specific action based on the rolebinding or clusterrolebinding it is associated with.\nIn this blog we will be looking at two ways of creating KUBECONFIG file they are\n Certificate based Token based  Certificate based authentication Generate the private key file.\nopenssl genrsa -out new-user.key 2048 Create a certificate sign request new-user.csr using the private key you just created (new-user.key in this example). Make sure you specify your username and group in the -subj section (CN is for the username and O for the group). As previously mentioned, we will use new-user as the name and bitnami as the group:\nopenssl req -new -key employee.key -out new-user.csr -subj \u0026#34;/CN=new-user/O=aqua\u0026#34;openssl x509 -req -in new-user.csr -CA CA_LOCATION/ca.crt -CAkey CA_LOCATION/ca.key -CAcreateserial -out new-user.crt -days 500kubectl config set-credentials new-user --client-certificate=/home/new-user/.certs/new-user.crt --client-key=/home/new-user/.certs/new-user.keykubectl config set-context new-context --cluster=minikube --namespace=test --user=new-user After running the above steps we have successfully created a user with name as new-user belonging to aqua group. But this user doesn\u0026rsquo;t have any previleges by defaults nil privileges will be assigned to the user. We need to explicitly grant the access privilges required by the user.\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: test-user-full-access namespace: test rules: - apiGroups: [\u0026#34;\u0026#34;, \u0026#34;apps\u0026#34;,\u0026#34;extensions\u0026#34;] resources: [\u0026#34;pods\u0026#34;,\u0026#34;pods/log\u0026#34;,\u0026#34;deployments\u0026#34;] #resourceNames: [\u0026#34;nginx-deployment\u0026#34;] #Confines privileges to specified resource name verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;batch\u0026#34;] resources: - jobs - cronjobs verbs: [\u0026#34;*\u0026#34;]kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: test-rolebinding namespace: test subjects: - kind: ServiceAccount name: new-user namespace: test roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: test-user-full-access Token based authentication Token based authentication deals with service account. Firstly we create a service account which has internal RoleBinding or ClusterRoleBinding based on the requirement. And this has a secret token encoded in base64 and also a certificate. We need secret token and certificate to authenticate with cluster. We rovide this values in KUBEONFIG file.\n Creating a serviceaccount in test namespace.\napiVersion: v1 kind: ServiceAccount metadata: name: test-user namespace: test Creating a clusterrole with privileges\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: test-user-full-access rules: - apiGroups: [\u0026#34;\u0026#34;, \u0026#34;apps\u0026#34;,\u0026#34;extensions\u0026#34;] resources: [\u0026#34;pods\u0026#34;,\u0026#34;pods/log\u0026#34;,\u0026#34;deployments\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;batch\u0026#34;] resources: - jobs - cronjobs verbs: [\u0026#34;*\u0026#34;] Creating a clusterrolebinding by binding a serviceaccount to it.\nkind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: test-rolebinding subjects: - kind: ServiceAccount name: test-user namespace: test roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: test-user-full-access Get the service account secret for the provided namespace\nkubectl describe sa test-user -n test Get the service account token\nkubectl get secret test-user-token-xxxxx -n test -o \u0026#34;jsonpath={.data.token}\u0026#34; | base64 -d Get the certificate authority\nkubectl get secret test-user-token-xxxxx -n test -o \u0026#34;jsonpath={.data[\u0026#39;ca\\.crt\u0026#39;]}\u0026#34;  Template for KUBECONFIG file.\nAdd the kubernetes api endpoint, certificate and token in the below KUBECONFIG file.\napiVersion: v1 clusters: - cluster: certificate-authority-data: PLACE CERTIFICATE HERE server: https://YOUR_KUBERNETES_API_ENDPOINT name: kind contexts: - context: cluster: kind namespace: test user: test-user name: test current-context: test kind: Config preferences: {} users: - name: test-user user: client-key-data: PLACE CERTIFICATE HERE token: PLACE USER TOKEN HERE We have successfully created a KUBECONFIG file for a new user.\nOther way of understanding RBAC is to install helm in your kubernetes cluster First install helm by following helm official docs https://helm.sh/docs/using_helm/#installing-helm\nWe are creating ServiceAccount with name tiller\napiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system We are binding the cluster admin privileges tiller service account\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system After creating the above ServiceAccount and ClusterRoleBinding\nRun the tiller\nhelm init --service-account tiller Check the helm installation and validate ServiceAccount by running following command\nhelm install stable/mysql By this you have successfully configured helm in your cluster which needs special privileges to communicate with API-SERVER.\nNote:\n By default when a new namespace is created a default ServiceAccount specific to that namespace is created but this serviceaccount has no authorization privileges all the resources running in this namespace will use the default ServiceAccount, Unless we need any privileges specific to a resource we need to specify it in resource.yaml as mentioned in below example.\napiVersion: v1 kind: Pod metadata: name: nginx-creator namespace: test spec: serviceAccountName: nginx-creator #This resource has special privileges binded in this ServiceAccount containers: - name: nginx-creator image: vineeth97/nginx-pod-creator We can also bind a ClusterRole to a RoleBinding which provides privileges to subject provided in RoleBinding but the scope is specific to namespace mentioned in the RoleBinding.\n  Slides prepared for my talk on RBAC can be found here\nCheers!\n","permalink":"https://vineethweb.com/blog/k8s-rbac/","tags":["kubernetes"],"title":"Limiting Privileges In Kubernetes"},{"categories":null,"contents":" \nProject Abstract The Postgres Operator is a project to create an open-sourced managed Postgres service for Kubernetes. The Postgres operator manages Postgres clusters on Kubernetes. kubectl plugins enable extending the Kubernetes command-line client kubectl with commands to manage custom resources. The task is to design and implement a plugin for the kubectl postgres command. My project aims to simplify and ease the usage of postgres clusters using the kubectl plugin. As the postgres operator is capable of many features having a kubectl plugin will ease in running the clusters and u nderstanding the resources way better.\nDescription The developed kubectl pg plugin helps in managing the postgres clusters efficiently and by decreasing the efforts in working with postgresql resources. Usually dealing with custom resource definations is a bit complicated as everythng needs to verfied and updated manually by changing the manifest file. kubectl pg plugin helps in managing the postgres clusters with ease. We can check whether the postgresql CRD is registered in the cluster or not. By this we can go ahead in creating CRD resources without any manual verfication such as state of postgres operator and CRD created by it. Dealing the postgres-operator is challenging when we have multiple operators in different namespaces. I developed a command which shows the current version of postgres-operator in current namespace and also in specified namespace. Though the usual way of creating postgres resources is using kubectl apply command for ease in managing postgres cluster individually we created kubectl pg create cmd specific to postgres resources. The same has been implemented for update as kubectl pg update and for delete as kubectl pg delete.\nDemo \nDeveloped Features: \nCheck whether the postgres CRD is registered. This makes sure CRD is installed in the kubernetes cluster. which helps us in creating postgresql resources\nkubectl pg check Create postgres cluster using manifest file This is an alternative to kubectl apply but built specifically to handle creation of kind postgresql resources.\nkubectl pg create -f manifest.yaml Update postgres cluster using manifest file This is an alternative to kubectl apply but built specifically to handle updation of kind postgresql resources.\nkubectl pg update -f manifest.yaml Delete postgres cluster using manifest file This is an alternative to kubectl delete but built specifically to handle deletion of kind postgresql resources and verifies the deletion with confirmation.\nkubectl pg delete -f manifest.yaml Delete postgres cluster using cluster name This is built specifically to delete postgresql cluster using it\u0026rsquo;s name in the current namespace.\nkubectl pg delete cluster Delete postgres cluster using cluster name in specified namespace This is built specifically to delete postgresql cluster using it\u0026rsquo;s name in the provided namespace.\nkubectl pg delete cluster -n namespace List postgres cluster from current namespace This feature helps in listing the postgres clusters in the current namespace.\nkubectl pg list List postgres clusters from all namespaces This feature helps in listing the postgres clusters from all the namespaces.\nkubectl pg list -A Extend volume of an existing cluster This feature let\u0026rsquo;s you extend the size of the volume.\nkubectl pg ext-volume 2Gi -c cluster Scale the number of instances of postgres cluster This feature let\u0026rsquo;s to scale up and down by providing the desired instances.\nkubectl pg scale 10 -c cluster Add a database and it\u0026rsquo;s owner to a postgres cluster This feature let\u0026rsquo;s you add new database and associated owner.\nkubectl pg add-db DB01 -o OWNER -c cluster Add a user and set of privileges to a postgres-cluster This feature let\u0026rsquo;s you add new user and privileges such as superuser, inherit ,login, nologin, createrole, createdb, replication, bypassrls.\nkubectl pg add-user USER01 -p login,createdb -c cluster Fetch the logs of the postgres operator Get the logs of postgres-operator pod.\nkubectl pg logs -o Fetch the logs of the postgres cluster  Get the logs of random replica from the provided cluster.\nkubectl pg logs -c cluster Get the logs of master pod from the provided cluster.\nkubectl pg logs -c cluster -m Get the logs of specified replica from the provided cluster.\nkubectl pg logs -c cluster -r 3  Connect to shell prompt  Connect to the shell prompt of random replica.\nkubectl pg connect -c cluster Connect to the shell prompt of master pod in the provided cluster.\nkubectl pg connect -c cluster -m Connect to shell prompt of specified replica for the provided postgres cluster.\nkubectl pg connect -c cluster -r 2  Connect to psql prompt of random replica  Connect to the psql prompt of random replica with db-name as current user and db-user as current user.\nkubectl pg connect -c cluster -p Connect to the psql prompt of random replica with db-user as specified user and db-name as specified user.\nkubectl pg connect -c cluster -p -u user01 Connect to psql prompt of random replica with provided postgres cluster, db-user as specified user and db-name as specified db-name.\nkubectl pg connect -c cluster -p -u user01 -d db01 Connect to psql prompt of specified replica for the provided postgres cluster, db-user as current user and db-name as current username.\nkubectl pg connect -c cluster -p -r 4 Connect to psql prompt of specified replica with provided postgres cluster, db-user as specified user and db-name as specified user.\nkubectl pg connect -c cluster -p -r 3 -u user01 Connect to psql prompt of specified replica with provided postgres cluster, db-user as specified user and db-name as specified db-name.\nkubectl pg connect -c cluster -p -r 3 -u user01 -d db01 Connect to psql prompt of master for the provided postgres cluster, db-user as current user and db-name as current username.\nkubectl pg connect -c cluster -p -m Connect to psql prompt of master with provided postgres cluster, db-user as specified user and db-name as specified user.\nkubectl pg connect -c cluster -p -m -u user01 Connect to psql prompt of master with provided postgres cluster, db-user as specified user and db-name as specified db-name.\nkubectl pg connect -c cluster -p -m -u user01 -d db01  Version details of kubectl pg plugin and postgres-operator  Get the version of kubectl pg plugin and postgres-operator in default namespace.\nkubectl pg version Get the version of kubectl pg plugin and postgres-operator in specified namespace.\nkubectl pg version -n namespace  ","permalink":"https://vineethweb.com/projects/contributions/kubectl-pg-plugin/","tags":["kubernetes"],"title":"kubectl pg plugin for Zalando's Postgres Operator"},{"categories":null,"contents":" \nKubernetes Kubernetes only understands pods not containers.\nPod Creation Requests  Bare pod RelplicaSet Deployment  Multi Container Pods  Share access to memory space Connect to each other using localhost Share access to the same volumes (storage abstraction) Tightly coupled. One crashes, all crashes. Same parameters such as config maps.  Pod Atomic Unit in Kubernetes is Pod\nIn a pod either all conatiners or none of them run. It\u0026rsquo;s always a pod runs on single node. Which pod runs on which node decided be scheduler. If a pod is failed kubelet notifies to k8s control plane.\nHigher level Kubernetes Objects Replicaset, ReplicationController: Scaling and healing Deployment: Versioning and rollback Service: Static (non-epemeral) IP and networking Volume: Non-ephemeral storage\nK8s nodes contains  Kubelet Kube proxy Conatiner Runtime.  Pod cannot be auto scaled or self healed for tis cases we need higher level objects such as ReplicaSet, ReplicationController, Deployment.\nMicro-Services Bare Metal: Apps ere very tightly coupled. Virtual Machines: Less tighly coupled, but not yet micro-services. Containers for Micro-Services: Simple, Independent components.\nResource in Conatiners  Usually the conatianers are aloocated with deafult resources in kubernetes. By providng Resource Request we are seeking resources as requested to be default resources. By providing limit in resources. You are restricting conatiner to occupy till the limit mentioned starting from default/Requested resources. You can even mention both request and limit or either of them or neither of them.  Communicaion between Master and Cluster Kubernetes Master Conatins following components\n Kube-scheduler Controller-Manager this is based n where k8s cluster is running. If it is not on cloud provider then Kube-controller manager, If it is cloud then cloud contoller manager.  Cloud controller Manager Kube-controller Manager  etcd Kube-apiserver  Cluster to Master Only APISERVER will be exposed outside(i.e to cluster) none of the other components are exposed outside.\nAll cluster to master communication happen with only API-SERVER.\nRelatively Secure\nMaster to Cluster  APISERVER to Kubelet  These are not safe in public or untusted networks\n Certificate not verified by default. Vulnerable to man in the middle attacks. Dont run on public network To hardern.\n set- -kubelet-certificate-authority Use SSH tunneling.\n  APISERVER to nodes/pods/Services\n Not safe Plain HTTP Neither authenticated or encrypted. On public clouds, SSH tunneling provided by cloud provider e.g. GCP.    Where can we run kubernetes  Public Clouds\n AWS Azure GCP  Bootstrap for running k8s cluster on private cloud or on prem. Using kubeadm we configure the kubernetes.\n Playgrounds.\n PWK MINIKUBE    Hybrid, Multi-Cloud Hybrid: On-prem + Public Cloud\nMulti-Cloud: More than 1 public cloud.\nFederated Clusters  Nodes in multiple clusters. Administer with kubefed.  Individual cluster  All nodes on sae infra. Administer with kubectl.  Kubernetes Provides  Fault-tolerance: Pod/Node faiures Rollback: Advanced Deployment options Auto-healing: Crashed conatiner restart Auto-scaling: More clients? More demand Load-balancing: Distribute client requets Isolation: Sanboxes so that containers don\u0026rsquo;t interfere.  How to interact with kubernetes  kubectl: Most common command line utility. Makes POST requests to apiserver of control plane. kubeadm: Bootstrap cluster when not on cloud kubernetes service. To create cluster out of individual infra nodes. kubefed: Administer federated cluters. Federated cluster -\u0026gt; group of multiple clusters (multi-cloud,hybrid)  kubelet, kube-proxy,\u0026hellip;. these are different cmd line utilities to interact with different components of k8s cluster.\nKubernetes API  APISERVER within conrol plane exposes API endpoints CLients hit these endpoints with RESTful API calls. These clients could be command line tools such as kubectl, kubeadm\u0026hellip;.. Could also be programmatic calls using client libraries.  Objects  Kubernetes Objects are persistent entities. Everything is an object\u0026hellip;. Pod, RelplicaSet, Deployment, Node \u0026hellip;. all are objects Send object specification (usually in .yaml or json)  IMPORTANT POINTS  Pods doen\u0026rsquo;t support auto-healing or auto scaling. Kube-apiserver - Accepts incming HTTP post requests from users. Etcd - Stores metadata that forms the state of the cluster. Kube-scheduler - Makes Decision about where and when the pods should run. CLoud-controller manager - Keeps the actual and desired state of the cluster in synch.  Three object Management Methods  Imperative Commands No .yaml or config files eg: kubectl run \u0026hellip;, kubectl expose \u0026hellip;, kubectl autscale .. For this happen the objects should be live in cluster and this is the least robust way of managing objects.\n Imperative Object Configuration kubectl + yaml or config files used. eg: kubectl create -f config.yaml kubectl replace -f config.yaml kubectl delete -f config.yaml\n Declarative Object Configuration Only .yaml or configfiles used eg: kubectl apply -f config.yaml This is the most preferred way of handling objects.\n  Note: Don\u0026rsquo;t mix and match different methods in handling k8s objects.\nImperative Commands kubectl run nginx --image nginx kubectl create deployment nginx --image nginx  No config file. Imperative: intent is in command.\n Pro:\n Simple  Cons:\n No audit trail or review mechanism Cant reuse or use in template.   Imperative Object Configuration kubectl create -f nginx.yaml kubectl delete -f nginx.yaml kubectl replace -f nginx.yaml config file required Still imperative: intent is in cmd.\nPros: -still simple -Robust - files checked into repo -One file for multiple operations\nDeclarative Object Configuration used in production kubectl apply -f configs/\nconfig files are all that is required. Declarative not imperative.\nPros; -Most robust - review,repos,audit trails. -k8s will automatically figure out intents -Can specify multiple files/directories recursively.\nDeclarative Configuration has three phases.\n Live object configuration Current object configuration file. Last-applied object configuration file.  Merging changes.\n Primitive fields\n String, int, boolean,images or replicas Replace old state with current object configuration file.   Map fields\n Merge old state with current state with current object configuration file.  List fields\n Complex- varies by field.    VOLUMES AND PersistentVolumes  Volumes(in general): lifespan of abstraction = lifetime of pod.\n Note that this is longer than lifetime of any container inside pod. Persistent Volumes.   Persistent Volumes: lifetime of abstraction independent of pod lifetime.\n  Using Volumes\nkind: Pod metadata: name: configmap-pod spec: containers: - name: test image: bisybox volumeMounts: // Each container will mount independently - name : config-vol mountPath: /etc/config\t// different paths in each container. volumes: //Define volume in pod spec -name: config-vol configMap: name: log-config items: - key: log_level path: log_level Volumes binded to pod are persistent across the lifecyclces of containers. But when pod restarts the vlumes are last. emptyDir comes with empty volume initially and also when pod restarts it loses all the data in it.\nImportant types of volumes are  configMap emptyDir gitRepo secret hostPath  emptyDir This is not persistent. his exists as long as the pod exists. Created as empty volume. Share space/state across conatiners in same pod. When the pod is removed the emptyDir volume is lost.\nWhen pod removed/crashes. data lost When conatiner crashes data remains Usecases: Scartch space, checkpointing\nhostPath Mount file/directory from node filesystem into pod Uncommon - pods should be independent of nodes Makes pod-node coupling tight Usecases: Access docker internals, running cAdvisor BLock devices or sockets on host\ngitRepo This volume will create an empty directory and go ahead and clone git repo to our volume so that our conatiners can use it.\nconfigMap Used to inject paraeters and configuration data into pods. configMap volume mount data from configmap object configMap objects define key-value pairs configMap objects inject parameters into pods\nTwo main usecases: 1.Providing config information for apps running inside pods 2. Specifying config infrmation for control plane(controllers )\nkubectl create configmap fmap \u0026ndash;from-file=file1.txt \u0026ndash;from-file=file2.txt\napiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: special.how: very Inside pod yaml file env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how Secret Pass sensitive information to pods. You can store secrets using kubernetes api and mount those secrets as files these files will be available to use by pods. using the secret volume You should know secrets are backed by RAM based file system which ensures contents of this files are never written to non volatile storage.\napiVersion: v1 kind: Secret metadata: name: test-secret data: username: VINEETH password: ###@!# Once the secrets are created we can access from volumes inside the pod yaml file.\nspec: conatiners: - name: test-container image: nginx volumeMounts: // name must matc the voume name below - name: secret-volume mountPath: /etc/secret-volume // The secret data is exposed to containers in the Pod through a volume. volumes: - name: secret-volume secret: secretName: test-secret We can access this secret by getting into the container shell and by going to etc/secret-volume.\nWe can create secrets directly from files.\nkubectl create secret generic sensitive \u0026ndash;from-file=./username.txt \u0026ndash;from-file=./password.txt Inside pod yaml file\nenv: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: sensitive key: username.txt - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: sensitive key: password.txt Using PersistentVolumes we mount the persistnt volumes with containers\nvolumeMounts: - mountPath: /test-pd name: test-volume Conatiners in Pod  Configure Nodes to Authenticate to Private Repos. All pods can pull any image. Pre-pull images. Pods can only use cached images. ImagePullSecrets on each pod. Only pods with secret can pull secrets.  What Environment Do Containers See ?\n Filesystem Image(at root) Associated Volumes\n ordinary persistent   Container Hostname Hostname refers to the pod name in which conatiner is running. We can get by cmd hostname or gethostname function call from libc.\n Pod Pod Name User-defined environment variables using Downward API\n Services List of all services\n  Services for stable IP Addresses Service object - load balancer Service = Logical set of backend pods + stable front-end Front-end: Static clusterIP address + Port + DNS Name Back-end: Logical set of backend pods(label selector)\nSetting up environment varibales spec: conatiners: - name: envar-demo-container image: gcr.io/google-samples/node-hello:1.0 env: - name: DEMO value: \u0026#34;HELLO\u0026#34; - name: DEMO1 valueL \u0026#34;HEY\u0026#34; kubectl exec -it demo-pod \u0026ndash; /bin/bash This will take into the bash shell within our conatiner. printenv \\ will print all env variables.\nDownward API Passing information from pod to conatiner such as metadata, annotations.\npods/inject/dapi-volume.yaml\napiVersion: v1 kind: Pod metadata: name: kubernetes-downwardapi-volume-example labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22 annotations: build: two builder: john-doe spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en \u0026#39;\\n\\n\u0026#39;; cat /etc/podinfo/labels; fi; if [[ -e /etc/podinfo/annotations ]]; then echo -en \u0026#39;\\n\\n\u0026#39;; cat /etc/podinfo/annotations; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo readOnly: false volumes: - name: podinfo downwardAPI: items: - path: \u0026#34;labels\u0026#34; fieldRef: fieldPath: metadata.labels - path: \u0026#34;annotations\u0026#34; fieldRef: fieldPath: metadata.annotations In the above example we are making pod metadata such as labels, annotations available for conatiners. etc/podinfo/annotations annotations are available in this file. etc/podinfo/labels labels are available in this file.\nConatiner Lifecycle Hooks  Post Start  Called immediately after conatiner created No parameters\n Pre Stop  Immediately before conatiner terminates.\nBlocking - must complete before conatiner can be deleted. This is synchronous.\n Hook handkers\n Exec //This executes shell scripts by getting inside conatiner HTTP // We can make calls to specific endpoint on the conatiner\napiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: conatiners: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Hello from the postStart handler \u0026gt; /usr/share/message\u0026#34;] preStop: exec: command: [\u0026#34;/usr/sbin/nginx\u0026#34;,\u0026#34;-s\u0026#34;,\u0026#34;quit\u0026#34;]   Pod Node Matching How can pods be assigned to specific nodes?\nHandled by kube-scheduler -Quite smart (it makes sure the nodes which has resources gets the pod assigned.) Granular usecases: -specific hardware: SSD required by pod -Colocate pods on same node: they communicate a lot. -High-availability:force pods to be an different nodes.\nnodeSelector (nodes have predefined labels hostname, zone, OS, instance type\u0026hellip;) -Simple -Tag nodes with labels -Add nodeSelector to pod template -Pods will only reside on nodes that are selected ny nodeSelector -Simple but crude - hard constriant\nAffinity and Anti-Affinity Node Affinity (nodes have predefined labels hostname, zone, OS, instance type\u0026hellip;) -steer pod to node - can be \u0026lsquo;soft\u0026rsquo; -Only affinity (for anti-affinity use taints)\nPod Affinity -Steer pods towards or away from pods. -Affinity: pods close to each other -Anti-Affinity: pods away from each other.\napiVersion: v1 kind: Pod metadata: Pod name: nginx labels: env: test spec: conatiners: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd Taints and Tolerations Using nodeslector you can make sure this pod should run on specific node but using taints and tolerations you can make sure certain pods can only run on certain nodes.\nDedicated nodes for certain users - Taint subset of nodes - Only add tolerations to pods of those users. Nodes with special hardware - Taint nodes with GPUs - Add toleration only pods running ML jobs\nTaints based on Node Condition\n New feature - in Alpha in v1.8 Taints added by node controller  Taints added by node controller - node.kubernetes.io/memory-pressure - node.kubernetes.io/disk-pressure - node.kubernetes.io/out-of-disk\nPods with tolerations are scheduled on this nodes. This will happen if flag set on nodes - TaintNodesByCondition=true\nTo taint a node\nkubectl label deployments/nginx env=dev The above cmds makes sure the pods from the deployment are not schduled on tainted node because env=dev:NoSchedule the pods with this label will not be scheduled on the node.\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 7 selector: matchLabels: app: ngnix template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 tolerations: - key: \u0026#34;dev\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;env\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; The above deployment has toleration so the pods can even be schduled on tainted nodes.\n### Init Containers ###\n Run before app containers. Always run-to-completion Run serially (each only starts after previous one finishes) If init containers fails kubernetes will repeatedly restart the pod to succeed the init containers.  Usecases: - Run utilities that should run before app container. - Different namespace/isolation from app containers. - Security reasons. - Include utilities or setup (gitclone, register app) - BLock or delay start of app contianer.\nDownward API is used to share metadata from the pod to the container.\napiVersion: v1 kind: Pod metadata: name: init-demo spec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMunts: - name: workdir mountPath: /usr/share/nginx/html # These containers are run during pod initialization initContainers: - name: install image: busybox command: - wget - \u0026#34;-O\u0026#34; - \u0026#34;/work-dir/index.html\u0026#34; - http://google.com volumeMounts: - name: workdir mountPath: \u0026#34;/work-dir\u0026#34; dnsPolicy: Default volumes: - name:workdir emptyDir: {}  ### Pod Lifecycle\n Pending: Request accepted, but not yet fully created Running: Pod bound to node, all containers started Succeeded: All containers are terminated successfully (will not be restarted). Failed: All containers have terminated, and at least one failed. Unknown: Pod status could not be queried - host error likely.   Note: - Container within pod are deployed in an all or nothing manner. - Entire pod is hosted on the same node.\nRestart policy for conatiners in a Pod.\n Always (default) On-failure Never   Probes Kubelet sends probes to containers\nAll succeeded? Pod status = Succeeded Any failed? Pod status = Failed Any running? Pod status = Running\nLiveness Probes  Failed? Kubelet assumes container dead( This probe certifies that pod is running else retries until the probe succeeds.) Restart policy will kick in.  Usecase: Kill and restart if probe fails. Add liveness probe, Specify restart plicy of Always or On-Failure.\nReadiness Probes  Ready to service requests? Failed? Endpoint object will remove pod from services.  Usecase: Send traffic only after probe succeeds. Pod goes live, But will only accept traffic after readiness probe succeeds. This is also referred as \u0026ldquo;Container that takes itself down\u0026rdquo;.\napiVersion: v1 kind: Pod metadata: labels: test: livesness namee: livesness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 //it says to wait 5 sec before starting first probe. periodSeconds: 5 // kubelet will perform liveness probe for every 5 seconds.  In the above pod livenessProbe has a cmd if cmd fails it will go ahead and kill the container.\nPod Presets Pod Presets are way to inject values during pod creation using labels which makes them loosely coupled. Values we pass may involve secrets, volumes, voumeMounts and environment variables.\nPod Priorities Create PriorityClass Object\napiVersion: v1appha1 kind: PriorityClass metadata: name: high-priority value: 1000000 globalDefault: false description: \u0026#34;XYZ\u0026#34; Reference from Pod Spec\napiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent priorityClassName: high-priority  Scheduling Order High-priority pod can \u0026lsquo;jump the queue\u0026rsquo;.\nPreemption  Low-priority pod maybe pre-empted to make way( if no node currently available to run gigh-priority pod). Preempted pod gets a graceful termination period.  ReplicaSets Pod - Containers inside pod template\nReplicaSet: - pod template - number of replicas - self-healing and scaling\nDeployment: - Conatins spec of ReplicaSet within it - Versioning - Fast rollback - Advanced deployments\napiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: guestbook tier: frontend spec: replicas: 3 selector: matchLabels: tier: frontend matchExpressions: - {key: tier, operator: In, value: [frontend]} template: metadata: labels: apps: guestbook tier: frontend spec: containers: - name: php-redis image: hello:v3 ports: - containerPort:80 Deleting ReplicaSets\n Deleting RelicaSet and its Pods\n Use kubectl delete   Deleting just ReplicaSet but not its Pods\n Use kubectl delete \u0026ndash;cascade=false   Deleting ReplicaSet orphans its pods\n Pods are now vulnerable to crashes   Probably want a new RelicaSet to adopt them\n pod template will not apply.    Auto-Scaling a ReplicaSet\n Horizontal Pod Autoscaler Target\napiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: frontend-scaler spec: scaleTargetRef: kind: ReplicaSet name: frontend minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 50  Control-loop to track actual and desired CPU utilisation in pod. Target: ReplicationCOntroller, Deployments, ReplicaSets Policy CPU utilisation or custom metrics Won\u0026rsquo;t work with non scaling objects: DaemonSets    Working with Horizontal Pod AutoScalers\n$kubectl create hpa $kubectl get hpa $kubectl describe hpa $kubectl autoscale rs front-end --min=3 --max=10 --cpu-percent=50 Thrashing is always a risk with autoscaling. i.e immediate scale up and down based on target metrics. Cooldown periods help HPA avoid this -horizontal-pod-autoscaler-downscale-delay -horizontal-pod-autoscaler-upscale-delay\nkubectl delete pods --all But replicaset will create the deleted pods from the above cmd pods associated with is label.\nTo delete this pods completely. Use the below cmd\nkubectl delete rs/frontend To remove the replicationSet controller on the pods.\nkubectl delete rs/frontend --cascade=false This will not delete the pods but the association between replicationSet and pods is detached. After this operation pods will not be created on deleton. They are vulnerable to crashes. As they are not governed by replicaset. This will delete the replicaSet object.\nkubectl get rs //This will shows no replicaSet as its deleted\nReplicaSets are lossely couple by the labels. As they are binded using labels. We can delete replicaset without touching underlying pods. We can even isolate the pods from the replicaSet by changing the labels.\nTo modify the live running pod run below cmd. By this we can detach the live pod from repicaset by changing the label. After this replicaset will create again detached pods as it always works for desired state.\n$KUBE_EDITOR=\u0026#34;nano\u0026#34; kubectl edit pod frontend-2d5b4 // Scaling replicaSet object $nano frontend.yaml //modify replicas field to desired number. $kubectl apply -f frontend.yaml // This will apply the modified changes to existing replicaset. But not good practice. Deployments  Deployments are the important objects in kubernetes. Usually deployments are used in production abd they comprise of replicaset template within them. When we use deployments we don\u0026rsquo;t directly work with pods or replicaset objects.\n When a container version inside a deployment object is updated. The new replicaset and new pods are created. Old replicaset continues to exist. Pods in old replicaset gradually reduced to zero.\n  Deployment objects provide - Versioning - Instant rollback - Rolling deployments - Blue-green - Canary deployments\nDeployment Usecases\n Primary usecase: To rollout a ReplicaSet( create new pods) Update state of existing deployment: just update pod template  new replicaset created, pods moved over in a controlled manner.  Rollback to earlier version: simply go back to previous revision of deployment. Scale up: edit number of replicas. Pause/Resume deployments mid-way (after fixing bugs) Check status of deployments (using the status field) Clean up old replicasets that are not needed any more.  Fields in Deployment\n Selector: Make sure the selector labels in the deployment are unique in every other deployment. This selector label is used replicaset to govern its pods. Strategy: How will old pods be replaced.  .spec.strategy.type == Recreate .spec.strategy.type == RollingUpdate  More Hooks for Rolling Update:\n .spec.strategy.rollingUpdate.maxUnavailable // This will make only specific number of pods to be deleted at a particular instance. Can be mentioned in number or percentage of pods. .spec.strategy.rollingUpdate.maxSurge   progressDeadlineSeconds // This tells the kubernetes how long should it wait before confirming it as failed.\n minReadySeconds\n rollbackTo\n revisionHistoryLimit\n paused.\n  Rolling back Deployment\n New revisions are created for any change in pod template  These changes are trivial to roll back.  Other changes to manifest: eg. scaling do not create new revision  Can not roll back scaling easily.    kubectl apply -f foo.yaml \u0026ndash;record // The flag \u0026ndash;records the changes made to specific object. kubectl rollout history deploymentname // This gives the history of changes applied on specific deployment. With revision number kubectl rollout undo deployment/nginx-deployment // will undo the rollout. kubectl rollout undo deployment/nginx-deployment \u0026ndash;to-revision=2 // This the revision you want to roll back to. This revision number can be obtained by cmd kubectl rollout history deploymentname.\nPausing and Resuming Deployments.\nImperative kubectl resume/pause commands\n$kubectl rollout resume deploy/nginx-deployment deployment \u0026#34;nginx\u0026#34; resumed $kubectl rollout pause deployment/nginx-deployment deployment \u0026#34;nginx-deployment\u0026#34; paused $kubectl rollout status deployment/nginx-deployment Declarative: Change spec.Paused boolean - Does not change pod template. - Does mot trigger revision creation.\n Can make changes or debug while paused. Changes to pod template while paused will not take effect until resumed. Can not rollback paused deployment need to resume it first.  Clean-Up Policy  Important: Don\u0026rsquo;t change this unless you understand it. Replicasets associated with deployment  New Replicaset for each revision So, one Replicaset for each change to pod template. Over period of time. We end up with so many revisions. We can clear them up or we can maintain desired number of older revisions. .spec.revisionHistoryLimit controls how many such revisions kept.  Setting .spec.revisionHistoryLimit = 0 c;eans up all history, no rollback possible.  Scaling Deployments  Imperative: kubectl scale commands\nkubectl scale deployments nginx-deployment --replicaa=10 deployment \u0026#34;nginx-deployment\u0026#34; scaled Declarative: Change number of repplicas and re-apply\n Scaling does not change pod template. So does not trigger creation of a new version. Can\u0026rsquo;t rollback scaling that easily.   Can also scale using horizontal pod autoscaler (HPA)\nkubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80 deployment \u0026#34;nginx-deployment\u0026#34; autoscaled\u0026#34;  Proportinate Scaling\n During rolling deployments, two ReplicaSets exist  old version new version  Proportinate scaling will scale pods in both ReplicaSets.  Imperative way of scaling\nkubectl scale deployments nginx-deployment --replicas=3 Declarative way of scaling\nBy editing yaml file and updating replicas field and kubectl apply -f name.yaml will scale declaratively.\nImperative way of changing the image version\nkubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 ### Stateful Sets\n Manage Pods Maintians a sticky identity Pods are created from the same spec Not interchangable Identifier maintains across any rescheduling.  Use cases\n Ordered, graceful deployment and scaling Ordered, graceful deletion and termination. Ordered, automated rolling updates. Stable, unique network identifiers. Stable, persistent storage.  Limitations:\n Pod must either be provisioned by a PersistentVolume Provisioner or k8s admin. Deleting and/or scaling a StatefulSet down will not delete the volumes associated with the statefulSet. StatefulSets currently require a Headless Service.  Deployment and Scaling Guarantees.\n N replicas, when Pods are being deployed, created sequentially, in order from (0\u0026hellip;N-1) When Pods are being deleted, they are terminated in reverse order from (N-1\u0026hellip;0) Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready. Before a Pod is terminated, all of its successors must be completely shutdown.\napiVersion: v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx\u0026#34; replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx port: 8080   In statefulsets pods have sequentially naming no random generation of names.\nDaemonSet  As we add nodes to the k8s cluster this type pods are also added to node. In precise they are bacckground processes such log collection etc\u0026hellip; Deleting the daemon set will clear the pods it has created.  Usecases - Cluster storage daemons - log Collection daemons - node monitoring darmons.\nThere are alternatives to daemon sets by directly creating daemon process on nodes by initialisation scripts. Satic pods these are controlled by kubelet they are not handled by api server or using kubectl.\nCron-Jobs Pods that do their job, then go away. - Create pods - Track their completion. - Ensure specified number terminate successfully. - Deleting job cleans up pods.\nTypes of Jobs - Non-parallel jobs: Can use to force 1 pod to run successfully. - Parallel jobs with fixed completion count: Job completess when number of completions reaches target. - Parallel jobs with work queue: Requires coordination.\nTracking Pods of Jobs - Once completed: no more pods created - Existing pods not deleted. - State set to terminated. - Can find them using kubectl show pods -a - You can delete them after listing them using above cmd.\nIf pods keeps failing, jobs keep creating. this leads to infinite loop. Use spec.activeDeadlineSeconds field to prevent this. The job will be ended after the mentioned time.\nUsecases -Manages time based job - Once at a specified point in time. - Repeatedly at a specified point in time. -Schedule a job execution at a given point in time.\nLimitations: - Jobs should be idempotent - Only responsible for creating jobs that match its schedule.\nBatch Processing apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: per1 command: [\u0026#34;perl\u0026#34;, \u0026#34;-Mbignum=bpi\u0026#34;, \u0026#34;-wle\u0026#34;, \u0026#34;print bpi(2000)\u0026#34;] restartPolicy: Never backoffLimit: 4 kubectl get pods --show-all we need to use \u0026ndash;show-all flag as the job objects come into existence execute its payload and gets completed.\nServices  Pod IP addresses keep changing as they go down and come up. For instance, when RepplicaSets or Deployments take pods up/down, IP addresses will change. Services help in maintaining stable network to the group of pods.  Types of Services  ClusterIP:\n Statis lifetime IP of service. Service only accesible within cluster. ClusterIP address is independent of nackend pods. Default type of service. Created by default even for NodePort, LoadBalancer service objects   NodePort:\n Service will also be exposed on each node on static port. External clients can hit Node IP + NodePort Request will be relayed/redirected to clusterIP + NodePort   LoadBalancer:\n External loadbalancer object Use LBs provided by AWS, GCP, Azure\u0026hellip; Will automatically create NodePort and ClusterIP services under the hood. External LB -\u0026gt; NodePort -\u0026gt; ClusterIP -\u0026gt; Backend Pod.   ExternalName:\n Map service to external service residing outside the cluster. Can only be accessed via kube-dns. No selectors in service spec.   Networking in Pods and Containers Docker - Host-private private networking - Ports must be allocated on node IPs - Containers need to allocate ports. - Burden of networking lies on containers.\nKubernetes - Pod can always communicate with each other - Inter- pod communication independent of nodes - Pods have private IP addresses(within cluster) - Containers within pod: use localhost - Containers across pods: pod IP address.\nService = Logical set of backend pods + stable front-end Front-end: Static IP address+ Port+DNS Name Back-end: Logical set of backend pods(label selector)\nClusterIP\n When service object created, ClusterIP is assigned Tied to service object through lifetime. Independent of lifespan of any backend pod. Any other pods can talk to CLusterIp and always access backend Service objects also has a static port assigned to it.  How labels are matched between pods and service objects.\nService object\nselector: matchLabels: tier: frontend matchExpressions: - {key: tier, operator: In, values: [frontend]}  Pod Object\nLabels { tier: frontend, env: prod, geo: India } Endpoint Object\n Dynamic list of pods hat are selected by a service. Each service object has an associated endpoint object. Kubernetes evaluates service label selector vs all pods in cluster. Dynamic list is updated as pods are created/deleted.  No selector - No Endpoint Object\n No endpoint object created  Need to manually map the service to specific IP or address.  ExternalName service: this is a service with no selector, no port  alias to external service in another cluster.   Services for STable IP Addresses\nFrom Within Cluster\n Endpoint object Dynamic list of pods Based on label selection  From Outside Cluster\n Virtual IP Can be accessed via any Node IP Node will relay to clusterIP  Multi-Port Services  Simply add multiple ports in the servie spec Each port must be named\n will have DNS SRV record\nkind: Service apiVersion:: v1 metadata: name: my-service spec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 - name: https protocol: TCP port: 443 targetPort: 9377   Service Discovery  Say a pod knows it needs to access some service. How do containers in that pod actually go about doing so? This is called Service Discovery Two methods:  DNS lookup: Preferred Environment Variables.    DNS Service Discovery  Requires dns add-on DNS server listens on creation of new services. When new service object created, DNS records created. All nodes can resolve service using name alone.  DNS Service Discovery of ClusterIP  Service name: my-service, Namespace: my-namespace Pods in my-namespace: simply DNS name lookup my-service. Pods in other namespaces: DNS name lookup my-service.my-namespace DNS Name lookup will return CLusterIP of service.  DNS lookup  Dynamic Preferred Requires DNS add-on  Environment Variables  Static Kubelet configures env variables for containers. Each service has environment variables for  host port Static - not updated after pod creation.    Headless Service Usually a clusterIP is created only once no matter how many ever pods come and go. This is static.\n Service without CLuster IP = Headless service Use if you don\u0026rsquo;t need  Load balancing. cluster IP.  Headless with selector? Associate with pods in this cluster. Headless without selector? Forward to ExternalName services  resolution for service in another cluster.    RBAC (Role based Access Control) Identity and Access Management (IAM)  Identities\n Individual Users(for users) Groups(for users) Service Accounts( not for humans)   Access\n RBAC ACLs(Access Control List)    RBAC has two types of Roles\n Roles: They govern the permissions for set of resources within namespace. ClusterRoles: Apply across entire cluster, All namespaces in cluster.  There are two types of bindings This are used to bind the identities and access - RoleBinding: Bind to specific namespace, Can bind either Role or ClusterRole. - ClusterRoleBinding: Bind across entire clutser, all namespaces in cluster, Can bind either Role or ClusterRole.\nA role contains rules that represent a group of permissions.\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default //Applicable to default namespace name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] // only for pods objects verbs: [\u0026#34;gets\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] // this are actions can be performed. As the above object is role its confined to a namespace. After creating the above object in k8s we don\u0026rsquo;t see any difference till we create rolebinding object.\nA ClusterRole can be used to grant the same permissions as a Role, but because ther are cluster scoped, they can also be used to grant access to:\n cluster-scoped resources (like nodes) non-resources endpoints(like \u0026ldquo;/healthz\u0026rdquo;) namespaced resources (like pods) across all namespaces (needed to run kubectl get pods \u0026ndash;all-namespaces, for example)\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: secret-reader rules: - apiGroups: [\u0026#34;\u0026#34;] - resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]  Once the role is created it can be bound to rolebind or clusterrolebinding. A rolebinding can be used by not only role but also clusterrole. The identities bound can be either users, groups or service accounts.\nkind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-pods namespace: default subjects: - kind: User name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: Role // You bind here cluster role as well but this role binding is applicable only to namespace that refered in metadata. name: pod-reader // This refers to the role with name pod-reader. apiGroup: rbac.authorization.k8s.io ClusterRoleBinding doesn\u0026rsquo;t include namespace field as it is applicable to the cluster as whole.\nkind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-secrets-global subjects: - kind: Group name: manager apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io API VERSION\u0026rsquo;s for specific objects \nKIND -\u0026gt; APIVERSION  CertificateSigningRequest ======================\u0026gt; certificates.k8s.io/v1beta1 ClusterRoleBinding ======================\u0026gt; rbac.authorization.k8s.io/v1 ClusterRole ======================\u0026gt; rbac.authorization.k8s.io/v1 ComponentStatus ======================\u0026gt; v1 ConfigMap ======================\u0026gt; v1| ControllerRevision ======================\u0026gt; apps/v1 CronJob ======================\u0026gt; batch/v1beta1 DaemonSet ======================\u0026gt; extensions/v1beta1 Deployment ======================\u0026gt; extensions/v1beta1 Endpoints ======================\u0026gt; v1 Event ======================\u0026gt; v1 HorizontalPodAutoscaler ======================\u0026gt; autoscaling/v1 Ingress ======================\u0026gt; extensions/v1beta1 Job ======================\u0026gt; batch/v1 LimitRange ======================\u0026gt; v1 Namespace ======================\u0026gt; v1 NetworkPolicy ======================\u0026gt; extensions/v1beta1 Node ======================\u0026gt; v1 PersistentVolumeClaim ======================\u0026gt; v1 PersistentVolume ======================\u0026gt; v1 PodDisruptionBudget ======================\u0026gt; policy/v1beta1 Pod ======================\u0026gt; v1 PodSecurityPolicy ======================\u0026gt; extensions/v1beta1 PodTemplate ======================\u0026gt; v1 ReplicaSet ======================\u0026gt; extensions/v1beta1 ReplicationController ======================\u0026gt; v1 ResourceQuota ======================\u0026gt; v1 RoleBinding ======================\u0026gt; rbac.authorization.k8s.io/v1| Role ======================\u0026gt; rbac.authorization.k8s.io/v1| Secret ======================\u0026gt; v1 ServiceAccount ======================\u0026gt; v1 Service ======================\u0026gt; v1 StatefulSet ======================\u0026gt; apps/v1 \nWhat does each apiVersion mean?\nalpha API versions with ‘alpha’ in their name are early candidates for new functionality coming into Kubernetes. These may contain bugs and are not guaranteed to work in the future.\nbeta ‘beta’ in the API version name means that testing has progressed past alpha level, and that the feature will eventually be included in Kubernetes. Although the way it works might change, and the way objects are defined may change completely, the feature itself is highly likely to make it into Kubernetes in some form.\nstable These do not contain ‘alpha’ or ‘beta’ in their name. They are safe to use.\nv1 This was the first stable release of the Kubernetes API. It contains many core objects.\napps/v1 apps is the most common API group in Kubernetes, with many core objects being drawn from it and v1. It includes functionality related to running applications on Kubernetes, like Deployments, RollingUpdates, and ReplicaSets.\nautoscaling/v1 This API version allows pods to be autoscaled based on different resource usage metrics. This stable version includes support for only CPU scaling, but future alpha and beta versions will allow you to scale based on memory usage and custom metrics.\nbatch/v1 The batch API group contains objects related to batch processing and job-like tasks (rather than application-like tasks like running a webserver indefinitely). This apiVersion is the first stable release of these API objects.\nbatch/v1beta1 A beta release of new functionality for batch objects in Kubernetes, notably including CronJobs that let you run Jobs at a specific time or periodicity.\ncertificates.k8s.io/v1beta1 This API release adds functionality to validate network certificates for secure communication in your cluster. You can read more on the official docs.\nextensions/v1beta1 This version of the API includes many new, commonly used features of Kubernetes. Deployments, DaemonSets, ReplicaSets, and Ingresses all received significant changes in this release.\nNote that in Kubernetes 1.6, some of these objects were relocated from extensions to specific API groups (e.g. apps). When these objects move out of beta, expect them to be in a specific API group like apps/v1. Using extensions/v1beta1 is becoming deprecated—try to use the specific API group where possible, depending on your Kubernetes cluster version.\npolicy/v1beta1 This apiVersion adds the ability to set a pod disruption budget and new rules around pod security.\nrbac.authorization.k8s.io/v1 This apiVersion includes extra functionality for Kubernetes role-based access control. This helps you to secure your cluster.\n","permalink":"https://vineethweb.com/blog/intro-to-k8s/","tags":["kubernetes"],"title":"Inspecting Kubernetes Theoritically"},{"categories":null,"contents":" \nThe main intention behind writing this blog is to understand docker instructions and it\u0026rsquo;s capabilities while writing a Dockerfile.\nIn the recent past it\u0026rsquo;s hard to find an application without a Dockerfile. 😃\n INSTRUCTIONS used in Dockerfile  FROM MAINTAINER RUN CMD LABEL EXPOSE ENV ADD COPY ENTRYPOINT VOLUME USER WORKDIR ARG ONBUILD STOPSIGNAL HEALTHCHECK SHELL   FROM FROM \u0026lt;image\u0026gt; # picks latest by default FROM \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; # specific version can be pulled FROM \u0026lt;image\u0026gt;@\u0026lt;digest\u0026gt; # This makes sure we pull the specific image with provided digest, # If image content is changed digest also changes.  FROM cmd can be used multiple times in the same Dockerfile to build multiple images. This generates unique Id for each image.\nfrom alpine:3.5 from python FROM vineeth97/saanvidashboard@sha256:ff6893d0750268ecfcdbe1e4a4d6f70b1a2ef43c5054ff11da0d5bc3595a79ec  MAINTAINER MAINTAINER \n This is deprecated as docker LABEL instruction does exactly the same.\nFROM alpine MAINTAINER vineeth  LABEL LABEL ABC=\u0026ldquo;XYZ\u0026rdquo; DEF=\u0026ldquo;MNO\u0026rdquo; LABEL ABC=\u0026ldquo;XYZ\u0026rdquo; DEF=\u0026ldquo;MNO\u0026rdquo; GHI=\u0026ldquo;PQR\u0026rdquo;\n The LABEL cmd adds metadata to an image. A LABEL is a key-value pair. If same key has different values the last occurring will take the precedence.\nFROM alpine:3.3 LABEL fruit=\u0026#34;APPLE\u0026#34; \\ vegetable=\u0026#34;TOMATO\u0026#34; \\ fruit=\u0026#34;BANANA\u0026#34; LABEL abc=\u0026#34;DEF\u0026#34; hjk=\u0026#34;PQR\u0026#34;  RUN RUN [\u0026#34;npm\u0026#34;,\u0026#34;start\u0026#34;] # exec form RUN npm start # shell form Difference between shell form and exec form. By default shell form executes the commands in /bin/bash -c context But exec form needs explicit context to be set. As it doesn\u0026rsquo;t pick /bin/bash -c by default.\nFROM ubuntu RUN apt-get update -y ENV myName=\u0026#34;John Doe\u0026#34; myDOg=Rex\\ The\\ Dog \\ myCat=fluffy ENTRYPOINT [\u0026#34;/bin/bash\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;echo $myName\u0026#34;] or ENTRYPOINT echo $myName The above docker instrctions are combination of shell and exec form in the ENTRYPOINT instruction.\nCMD CMD [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (exec form this is preferred form) CMD [\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (used to provide default parameters to ENTRYPOINT]) CMD \u0026lt;command\u0026gt; \u0026lt;param1\u0026gt; \u0026lt;param2\u0026gt;  CMD is used to provide a instruction to Dockerfile which also can overwritted using docker run cmd, Also used to provide defaults to the ENTRYPOINT cmd this defaults are also overwritted using docker run cmd .  ENTRYPOINT  Difference between CMD and ENTRYPOINT, CMD is used only once if it\u0026rsquo;s provided multiple times last provided one will take precedence. CMD is overwritted when we pass any command with docker run IMAGENAME XYZ ENTRYPOINT is used as the command which cannot be overwritten. But we can append parameters to the ENTRYPOINT either by CMD after ENTRYPOINT in Dockerfile or else by passing from docker run command.\nFROM ubuntu ENTRYPOINT [\u0026#34;echo\u0026#34;] CMD [\u0026#34;VINEETH\u0026#34;]  In the above docker instruction CMD instruction is taken by default if any string is provided with docker run IMAGENAME AQUA will override the default CMD. If no CMD is provided. Any command provided with docker run will be appended to ENTRYPOINT command.\nSHELL  The SHELL instruction is particularly used on windows. where there are two commonly used and quite different native shells: CMD and POWERSHELL. as well as alternate shells available including sh. The SHELL instruction can be used multiple times. But the previous SHELL instruction is ignored and current SHELL instruction affects all the subsequent instructions.\nFROM microsoft/windowsservercore # Executed as cmd /S /C echo default RUN echo default # Executed as cmd /S /C powershell -command Write-Host default RUN powershell -command Write-Host default # Executed as powershell -command Write-Host hello SHELL [\u0026#34;powershell\u0026#34;, \u0026#34;-command\u0026#34;] RUN Write-Host hello # Executed as cmd /S /C echo hello SHELL [\u0026#34;cmd\u0026#34;, \u0026#34;/S\u0026#34;, \u0026#34;/C\u0026#34;] RUN echo hello  HEALTHCHECK  HEALTHCHECK instruction is used to check the health of the container based on some validations. This lets us know whether the container is not only just up and running but also serving the requests as intended.\nFrom node:latest COPY ./ ./app EXPOSE 4200 WORKDIR /app RUN npm install #STOPSIGNAL SIGKILL #HEALTHCHECK CMD sleep 50 HEALTHCHECK --interval=1s --timeout=1s --retries=1 CMD curl --fail http://localhost:4200/ || exit 1 CMD [\u0026#34;npm\u0026#34;,\u0026#34;start\u0026#34;]  \u0026ndash;interval is used to check for time interval in checking the health \u0026ndash;timeout is used to limit the timeout for each check \u0026ndash;retries set retries to confirm unhealthy upon specific consecutive failures\nSTOPSIGNAL  STOPSIGNAL instruction is used to pass a signal to the container when we try stopping the container with docker stop it executes SIGTERM (this gives the graceful time to terminate the process if not it will kill the process) and docker kill executes SIGQUIT or SIGKILL.\nfrom ubuntu STOPSIGNAL SIGKILL CMD [\u0026#34;sleep\u0026#34;,\u0026#34;5000\u0026#34;]  ADD  ADD instruction is used same as COPY instruction but it has some additional functionalities such as ADD from remote URL, extracting the tar from the host machine to docker image during build without multiple instruction which in return creates the multiple layers besides from this functionalities it also does all functionalities COPY instruction does.\nfrom ubuntu RUN mkdir node ADD node-v8.10.0-linux-x64.tar.xz /node ADD https://nodejs.org/dist/v8.10.0/node-v8.10.0-linux-x64.tar.xz . CMD [\u0026#34;sleep\u0026#34;,5000\u0026#34;]  COPY  COPY instruction is used to copy data from host machine to container. With COPY instruction we specify the src and destination.\nfrom ubuntu COPY . . CMD [\u0026#34;sleep\u0026#34;,\u0026#34;5000\u0026#34;]  ENV  ENV instruction is used to pass the environment varibales to the container runtime. It works same as environment variables in host level.\nFROM ubuntu ENV myName=\u0026#34;John Doe\u0026#34; myDOg=Rex\\ The\\ Dog \\ myCat=fluffy ENTRYPOINT [\u0026#34;/bin/bash\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;echo $myName\u0026#34;]  ARG  ARG instruction is used to pass dynamic values during docker build with docker build \u0026ndash;build-arg VALUE=1.0\n ARG instruct can optionally contain default value if no value is passed during docker build. When both ARG and ENV is used with same name ENV will override the ARG.\nFROM ubuntu ARG app=nodejs RUN apt-get update \u0026amp;\u0026amp; apt-get install -y $app CMD [\u0026#34;sleep\u0026#34;,\u0026#34;5000\u0026#34;]  Above ARG instruction can we overrided during the build time using docker build \u0026ndash;build-arg app=python . In the above docker instructions app = nodejs, But this can be overrided using docker build.\nONBUILD  ONBUILD instruction is used to perform an action on the image which uses it as base image. Triggers are inherited by the \u0026ldquo;child\u0026rdquo; build only. In other words, they are not inherited by \u0026ldquo;grand-children\u0026rdquo; builds. The ONBUILD instruction may not trigger FROM, MAINTAINER, or ONBUILD instructions.\nfrom ubuntu ONBUILD RUN mkdir VINEETH  The above docker image will create VINEETH directory to all the images which use the above image as base image. i.e Build the above image using\ndocker build -t vineeth .from vineeth This image on build will perform the action which is mentioned in the base image with ONBUILD instruction.\nWORKDIR  WORKDIR instruction is used to predefine the current directory in which build CMD, RUN instructions needs to be executed on. Even when we bash into the container we get into the WORKDIR by default.\nfrom ubuntu RUN [\u0026#34;mkdir\u0026#34;,\u0026#34;sample\u0026#34;] WORKDIR sample/ ADD https://nodejs.org/dist/v8.10.0/node-v8.10.0-linux-x64.tar.xz . CMD [\u0026#34;sleep\u0026#34;,\u0026#34;5000\u0026#34;]  USER  USER instruction is used to set the default user to the container and this gets validated and impacted when we run the container. Even if a user is invalid build succeeds. This instruction teakes action during docker run step.\nfrom ubuntu RUN useradd -ms /bin/bash vineeth USER vineeth CMD [\u0026#34;sleep\u0026#34;,\u0026#34;500\u0026#34;]  Tips  During docker build the docker cli tries sending the entire content of Dockerfile directory to the docker daemon to increase the build’s performance, exclude files and directories by adding a .dockerignore file to the context directory. Starting with version 18.09, Docker supports a new backend for executing your builds that is provided by the moby/buildkit project. The BuildKit backend provides many benefits compared to the old implementation. To use the BuildKit backend, you need to set an environment variable DOCKER_BUILDKIT=1 on the CLI before invoking docker build.  ","permalink":"https://vineethweb.com/blog/docker-instructions/","tags":["Docker","Containers","DevOps"],"title":"All About Docker Instructions"},{"categories":null,"contents":" This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ]","permalink":"https://vineethweb.com/search/","tags":null,"title":"Search Results"},{"categories":null,"contents":"This project is a kubernetes resource scaler. The main objective of this project is to create 10\u0026rsquo;s/100\u0026rsquo;s/100\u0026rsquo;s of kubernetes resources with ease. Infact by just using a single command.\n","permalink":"https://vineethweb.com/projects/creations/k8s-scaler/","tags":["Kubernetes"],"title":"k8s-scaler to create k8s resources at scale"}]